{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTAR neomdel\n",
    "from neomodel import StructuredNode, StringProperty, RelationshipTo, RelationshipFrom, config, IntegerProperty, UniqueIdProperty, UniqueIdProperty\n",
    "\n",
    "#URL CONECCION CON LA BASE DE DARTOS DE NEO4J\n",
    "config.DATABASE_URL = 'bolt://neo4j:ExamenIA@localhost:7687'\n",
    "\n",
    "#CREAR Object Browsers\n",
    "class Browsers(StructuredNode):\n",
    "    nombre = StringProperty(unique_index=True)\n",
    "    presidente = RelationshipTo('PresidenteNoticias','BROWSERS PRESIDENT')\n",
    "    presidenteFacebook = RelationshipTo('PresidenteFacebook','BROWSERS PRESIDENT FACEBOOK')\n",
    "    asambleista = RelationshipTo('AsambleistaNoticia','BROWSERS ASAMBLEISTA')\n",
    "    asambleistaFacebook = RelationshipTo('AsambleistaFacebook','BROWSERS ASAMBLEISTA FACEBOOK')\n",
    "    asambleistasuplente = RelationshipTo('AsambleistaSuplente','ASAMBLEISTA SUPLENTE')\n",
    "    unes = RelationshipTo('UNES','UNES NOTICIA')\n",
    "    unesAzuay = RelationshipTo('UNESAZUAY','UNES AZUAY NOTICIA')\n",
    "    \n",
    "#CREAR Object PresidenteNoticias \n",
    "class PresidenteNoticias(StructuredNode):\n",
    "    url = StringProperty(unique_index=True)\n",
    "    nombre_Pagina_WEB = StringProperty(unique_index=True)\n",
    "    titulo = StringProperty(unique_index=True)\n",
    "    mensaje = StringProperty(unique_index=True)\n",
    "    fecha = StringProperty(unique_index=True)\n",
    "    browsers = RelationshipFrom('Browsers','BROWSERS PRESIDENT')\n",
    "\n",
    "#CREAR Object PresidenteFacebook \n",
    "class PresidenteFacebook(StructuredNode):\n",
    "    url = StringProperty(unique_index=True)\n",
    "    nombre_Pagina_WEB = StringProperty(unique_index=True)\n",
    "    titulo = StringProperty(unique_index=True)\n",
    "    mensaje = StringProperty(unique_index=True)\n",
    "    browsers = RelationshipFrom('Browsers','BROWSERS PRESIDENT FACEBOOK')\n",
    "\n",
    "#CREAR Object AsambleistaNoticia\n",
    "class AsambleistaNoticia(StructuredNode):\n",
    "    url = StringProperty(unique_index=True)\n",
    "    nombre_Pagina_WEB = StringProperty(unique_index=True)\n",
    "    titulo = StringProperty(unique_index=True)\n",
    "    mensaje = StringProperty(unique_index=True)\n",
    "    fecha = StringProperty(unique_index=True)\n",
    "    browsers = RelationshipFrom('Browsers','BROWSERS ASAMBLEISTA')\n",
    "\n",
    "#CREAR Object AsambleistaFacebook\n",
    "class AsambleistaFacebook(StructuredNode):\n",
    "    url = StringProperty(unique_index=True)\n",
    "    nombre_Pagina_WEB = StringProperty(unique_index=True)\n",
    "    titulo = StringProperty(unique_index=True)\n",
    "    mensaje = StringProperty(unique_index=True)\n",
    "    browsers = RelationshipFrom('Browsers','BROWSERS ASAMBLEISTA FACEBOOK')\n",
    "\n",
    "#CREAR Object AsambleistaSuplente\n",
    "class AsambleistaSuplente(StructuredNode):\n",
    "    url = StringProperty(unique_index=True)\n",
    "    nombre_Pagina_WEB = StringProperty(unique_index=True)\n",
    "    titulo = StringProperty(unique_index=True)\n",
    "    mensaje = StringProperty(unique_index=True)\n",
    "    fecha = StringProperty(unique_index=True)\n",
    "    browsers = RelationshipFrom('Browsers','ASAMBLEISTA SUPLENTE')\n",
    "\n",
    "#CREAR Object UNES\n",
    "class UNES(StructuredNode):\n",
    "    url = StringProperty(unique_index=True)\n",
    "    nombre_Pagina_WEB = StringProperty(unique_index=True)\n",
    "    titulo = StringProperty(unique_index=True)\n",
    "    mensaje = StringProperty(unique_index=True)\n",
    "    fecha = StringProperty(unique_index=True)\n",
    "    browsers = RelationshipFrom('Browsers','UNES NOTICIA')\n",
    "\n",
    "#CREAR Object UNESAZUAY  \n",
    "class UNESAZUAY(StructuredNode):\n",
    "    url = StringProperty(unique_index=True)\n",
    "    nombre_Pagina_WEB = StringProperty(unique_index=True)\n",
    "    titulo = StringProperty(unique_index=True)\n",
    "    mensaje = StringProperty(unique_index=True)\n",
    "    fecha = StringProperty(unique_index=True)\n",
    "    browsers = RelationshipFrom('Browsers','UNES AZUAY NOTICIA')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar los datos del object Browsers\n",
    "browsersG = Browsers(nombre = \"GOOGLE\").save()\n",
    "browsersE = Browsers(nombre = \"ECOSIA\").save()\n",
    "browsersB = Browsers(nombre = \"BING\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "#Palabras a Buscar \n",
    "#URL buscar a la candidata a asambleista Andre Gonzales \n",
    "asambleista = 'UNES+Andrea+Gonzalez'\n",
    "urlasambleista = 'https://www.google.com/search?q='+asambleista+'&start='\n",
    "#Rango de paginas a buscar a la candidata a asambleista Andre Gonzales \n",
    "rangoPaginasAsambleista = 9\n",
    "#URL buscar al candidato asambleista suplente Freddy Barros\n",
    "suplente = 'UNES+SUPLENTE+FREDDY+BARROS'\n",
    "urlsuplente = 'https://www.google.com/search?q='+suplente+'&start='\n",
    "#Rango de paginas a buscar al asambleista suplente Freddy Barros\n",
    "rangoPaginasSuplente = 9\n",
    "#URL buscar al candidato a presidente Andr√©s Arauz\n",
    "\n",
    "#URL buscar en noticias del candidato a presidente Andr√©s Arauz \n",
    "noticiasPresidente = 'https://www.google.com/search?q=Andres+Arauz+candidato&tbm=nws&start='\n",
    "#Rango de paginas a buscar por Noticias del candidato a presidente Andr√©s Arauz \n",
    "rangoNoticiasPresidente = 18\n",
    "\n",
    "\n",
    "#Encabezasos HTTP Para navegadores\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36'}\n",
    "\n",
    "cont=0\n",
    "contID = 1;\n",
    "contCambioPaginaP=0\n",
    "for i in range(rangoNoticiasPresidente):\n",
    "    url = noticiasPresidente+str(contCambioPaginaP)\n",
    "    respuesta = requests.get(url, headers = header )\n",
    "    contenido = BeautifulSoup(respuesta.content, \"html.parser\")\n",
    "    contenido = contenido.find_all('div', class_='dbsr')\n",
    "    \n",
    "    contCambioPaginaP=contCambioPaginaP+10\n",
    "    cont=cont\n",
    "    for lista in contenido:\n",
    "        cont=cont+1\n",
    "        \n",
    "        #SACAR URL\n",
    "        url = str(lista.find_all('a'))\n",
    "        url = url.split('href=\"')\n",
    "        url = str(url[1])\n",
    "        url = url.split('\" ping=\"')\n",
    "        url = str(url[0])\n",
    "        \n",
    "        #SACAR NOMBRE DE LA PAGINA WEB\n",
    "        nombreP =str(lista.find_all('div', class_='XTjFC WF4CUc'))\n",
    "        nombrePagina = nombreP.split('</g-img>')\n",
    "        nombrePagina = (nombrePagina[1])\n",
    "        nombrePagina = nombrePagina.replace('</div>]', '')\n",
    "        \n",
    "        #SACAR TITULO\n",
    "        titulo=str(lista.find_all('div', class_='JheGif nDgy9d'))\n",
    "        titulo=titulo.replace('[<div aria-level=\"2\" class=\"JheGif nDgy9d\" role=\"heading\" style=\"-webkit-line-clamp:2\">', '')\n",
    "        titulo=titulo.replace('</div>]', '')\n",
    "        \n",
    "        #SACAR MENSAJE\n",
    "        mensaje = str(lista.find_all('div', class_='Y3v8qd'))\n",
    "        mensaje=mensaje.replace('[<div class=\"Y3v8qd\">', '')\n",
    "        mensaje=mensaje.replace('</div>]', '')\n",
    "        \n",
    "        #SACAR FECHA\n",
    "        fecha=str(lista.find_all('span',class_='WG9SHc'))\n",
    "        fecha=fecha.replace('[<span class=\"WG9SHc\"><span>', '')\n",
    "        fecha=fecha.replace('</span></span>]', '')\n",
    "        #Guardar los datos del object presidenteNoticias\n",
    "        presidenteNoticias = PresidenteNoticias(url =url,nombre_Pagina_WEB=nombrePagina,titulo = titulo, mensaje = mensaje, fecha = fecha).save()\n",
    "        #Guardar los datos del object browsersG con relaciona al object presidenteNoticias\n",
    "        browsersG.presidente.connect(presidenteNoticias)\n",
    "        contID = contID + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "#Palabras a Buscar \n",
    "\n",
    "#URL buscar en noticias del candidato a presidente Andr√©s Arauz \n",
    "noticiasPresidente = 'https://www.google.com/search?q=Andres+Arauz+Candidato+Candidato+2021&tbm=nws&start='\n",
    "#Rango de paginas a buscar por Noticias del candidato a presidente Andr√©s Arauz \n",
    "rangoNoticiasPresidente = 18\n",
    "\n",
    "\n",
    "#Encabezasos HTTP Para navegadores\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36'}\n",
    "\n",
    "cont=0\n",
    "contID = 1;\n",
    "contCambioPaginaP=0\n",
    "for i in range(rangoNoticiasPresidente):\n",
    "    url = noticiasPresidente+str(contCambioPaginaP)\n",
    "    respuesta = requests.get(url, headers = header )\n",
    "    contenido = BeautifulSoup(respuesta.content, \"html.parser\")\n",
    "    contenido = contenido.find_all('div', class_='dbsr')\n",
    "    \n",
    "    contCambioPaginaP=contCambioPaginaP+10\n",
    "    cont=cont\n",
    "    for lista in contenido:\n",
    "        cont=cont+1\n",
    "        \n",
    "        #SACAR URL\n",
    "        url = str(lista.find_all('a'))\n",
    "        url = url.split('href=\"')\n",
    "        url = str(url[1])\n",
    "        url = url.split('\" ping=\"')\n",
    "        url = str(url[0])\n",
    "        \n",
    "        #SACAR NOMBRE DE LA PAGINA WEB\n",
    "        nombreP =str(lista.find_all('div', class_='XTjFC WF4CUc'))\n",
    "        nombrePagina = nombreP.split('</g-img>')\n",
    "        nombrePagina = (nombrePagina[1])\n",
    "        nombrePagina = nombrePagina.replace('</div>]', '')\n",
    "        \n",
    "        #SACAR TITULO\n",
    "        titulo=str(lista.find_all('div', class_='JheGif nDgy9d'))\n",
    "        titulo=titulo.replace('[<div aria-level=\"2\" class=\"JheGif nDgy9d\" role=\"heading\" style=\"-webkit-line-clamp:2\">', '')\n",
    "        titulo=titulo.replace('</div>]', '')\n",
    "        \n",
    "        #SACAR MENSAJE\n",
    "        mensaje = str(lista.find_all('div', class_='Y3v8qd'))\n",
    "        mensaje=mensaje.replace('[<div class=\"Y3v8qd\">', '')\n",
    "        mensaje=mensaje.replace('</div>]', '')\n",
    "        \n",
    "        #SACAR FECHA\n",
    "        fecha=str(lista.find_all('span',class_='WG9SHc'))\n",
    "        fecha=fecha.replace('[<span class=\"WG9SHc\"><span>', '')\n",
    "        fecha=fecha.replace('</span></span>]', '')\n",
    "        #Guardar los datos del object presidenteNoticias\n",
    "        presidenteNoticias = PresidenteNoticias(url =url,nombre_Pagina_WEB=nombrePagina,titulo = titulo, mensaje = mensaje, fecha = fecha).save()\n",
    "        #Guardar los datos del object browsersG con relaciona al object presidenteNoticias\n",
    "        browsersG.presidente.connect(presidenteNoticias)\n",
    "        contID = contID + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "#Palabras a Buscar \n",
    "#URL buscar a la candidata a asambleista Andre Gonzales \n",
    "asambleista = 'UNES+Andrea+Gonzalez'\n",
    "urlasambleista = 'https://www.google.com/search?q='+asambleista+'&tbm=nws&start='\n",
    "#Rango de paginas a buscar a la candidata a asambleista Andre Gonzales \n",
    "rangoPaginasAsambleista = 27\n",
    "#Encabezasos HTTP Para navegadores\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36'}\n",
    "\n",
    "cont=0\n",
    "contID = 1;\n",
    "contCambioPaginaA=0\n",
    "for i in range(rangoPaginasAsambleista):\n",
    "    url = urlasambleista+str(contCambioPaginaA)\n",
    "    respuesta = requests.get(url, headers = header )\n",
    "    contenido = BeautifulSoup(respuesta.content, \"html.parser\")\n",
    "    contenido = contenido.find_all('div', class_='dbsr')\n",
    "    \n",
    "    contCambioPaginaA=contCambioPaginaA+10\n",
    "    cont=cont\n",
    "    for lista in contenido:\n",
    "        cont=cont+1\n",
    "        \n",
    "        #SACAR URL\n",
    "        url = str(lista.find_all('a'))\n",
    "        url = url.split('href=\"')\n",
    "        url = str(url[1])\n",
    "        url = url.split('\" ping=\"')\n",
    "        url = str(url[0])\n",
    "        \n",
    "        #SACAR NOMBRE DE LA PAGINA WEB\n",
    "        nombreP =str(lista.find_all('div', class_='XTjFC WF4CUc'))\n",
    "        nombrePagina = nombreP.split('</g-img>')\n",
    "        nombrePagina = (nombrePagina[1])\n",
    "        nombrePagina = nombrePagina.replace('</div>]', '')\n",
    "        \n",
    "        #SACAR TITULO\n",
    "        titulo=str(lista.find_all('div', class_='JheGif nDgy9d'))\n",
    "        titulo=titulo.replace('[<div aria-level=\"2\" class=\"JheGif nDgy9d\" role=\"heading\" style=\"-webkit-line-clamp:2\">', '')\n",
    "        titulo=titulo.replace('</div>]', '')\n",
    "        \n",
    "        #SACAR MENSAJE\n",
    "        mensaje = str(lista.find_all('div', class_='Y3v8qd'))\n",
    "        mensaje=mensaje.replace('[<div class=\"Y3v8qd\">', '')\n",
    "        mensaje=mensaje.replace('</div>]', '')\n",
    "        \n",
    "        #SACAR FECHA\n",
    "        fecha=str(lista.find_all('span',class_='WG9SHc'))\n",
    "        fecha=fecha.replace('[<span class=\"WG9SHc\"><span>', '')\n",
    "        fecha=fecha.replace('</span></span>]', '')\n",
    "        #Guardar los datos del object AsambleistaNoticia\n",
    "        asambleistaNoticia = AsambleistaNoticia(url = url,nombre_Pagina_WEB= nombrePagina,titulo = titulo, mensaje = mensaje, fecha = fecha).save()\n",
    "        #Guardar los datos del object browsersG con relaciona al object AsambleistaNoticia\n",
    "        browsersG.asambleista.connect(asambleistaNoticia)\n",
    "        contID = contID + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "#Palabras a Buscar \n",
    "#URL buscar al candidato asambleista suplente Freddy Barros\n",
    "suplente = 'Freddy+Barros+asambleista'\n",
    "urlsuplente = 'https://www.google.com/search?q='+suplente+'&tbm=nws&start='\n",
    "#Rango de paginas a buscar al asambleista suplente Freddy Barros\n",
    "rangoPaginasSuplente = 30\n",
    "\n",
    "#Encabezasos HTTP Para navegadores\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36'}\n",
    "\n",
    "cont=0\n",
    "contID = 1;\n",
    "contCambioPagin=0\n",
    "for i in range(rangoPaginasSuplente):\n",
    "    url = urlsuplente+str(contCambioPagin)\n",
    "    respuesta = requests.get(url, headers = header )\n",
    "    contenido = BeautifulSoup(respuesta.content, \"html.parser\")\n",
    "    contenido = contenido.find_all('div', class_='dbsr')\n",
    "    \n",
    "    contCambioPagin=contCambioPagin+10\n",
    "    cont=cont\n",
    "    for lista in contenido:\n",
    "        cont=cont+1\n",
    "        \n",
    "        #SACAR URL\n",
    "        url = str(lista.find_all('a'))\n",
    "        url = url.split('href=\"')\n",
    "        url = str(url[1])\n",
    "        url = url.split('\" ping=\"')\n",
    "        url = str(url[0])\n",
    "        print(url)\n",
    "        \n",
    "        #SACAR NOMBRE DE LA PAGINA WEB\n",
    "        nombreP =str(lista.find_all('div', class_='XTjFC WF4CUc'))\n",
    "        nombrePagina = nombreP.split('</g-img>')\n",
    "        nombrePagina = (nombrePagina[1])\n",
    "        nombrePagina = nombrePagina.replace('</div>]', '')\n",
    "        print(nombrePagina)\n",
    "        \n",
    "        #SACAR TITULO\n",
    "        titulo=str(lista.find_all('div', class_='JheGif nDgy9d'))\n",
    "        titulo=titulo.replace('[<div aria-level=\"2\" class=\"JheGif nDgy9d\" role=\"heading\" style=\"-webkit-line-clamp:2\">', '')\n",
    "        titulo=titulo.replace('</div>]', '')\n",
    "        print(titulo)\n",
    "        \n",
    "        #SACAR MENSAJE\n",
    "        mensaje = str(lista.find_all('div', class_='Y3v8qd'))\n",
    "        mensaje=mensaje.replace('[<div class=\"Y3v8qd\">', '')\n",
    "        mensaje=mensaje.replace('</div>]', '')\n",
    "        print(mensaje)\n",
    "        \n",
    "        #SACAR FECHA\n",
    "        fecha=str(lista.find_all('span',class_='WG9SHc'))\n",
    "        fecha=fecha.replace('[<span class=\"WG9SHc\"><span>', '')\n",
    "        fecha=fecha.replace('</span></span>]', '')\n",
    "        print(fecha)\n",
    "        #Guardar los datos del object AsambleistaSuplente\n",
    "        asambleistasuplente = AsambleistaSuplente(url = url, nombre_Pagina_WEB= nombrePagina, titulo = titulo, mensaje = mensaje, fecha = fecha).save()\n",
    "        #Guardar los datos del object browsersG con relaciona al object AsambleistaSuplente\n",
    "        browsersG.asambleistasuplente.connect(asambleistasuplente)\n",
    "        \n",
    "        contID = contID + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "#Palabras a Buscar\n",
    "\n",
    "urlPresidenteEcosia = 'https://www.ecosia.org/news?q=andres%20arauz&p='\n",
    "rangourlPresidenteEcosia = 8\n",
    "\n",
    "\n",
    "#Encabezasos HTTP Para navegadores\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36'}\n",
    "\n",
    "cont=0\n",
    "contCambioPagina=0\n",
    "for i in range(rangourlPresidenteEcosia):\n",
    "    url = urlPresidenteEcosia+str(contCambioPagina)\n",
    "    respuesta = requests.get(url, headers = header )\n",
    "    contenido = BeautifulSoup(respuesta.content, 'html.parser')\n",
    "    conten = contenido.find_all('section', class_='news__results')\n",
    "    conten = contenido.find_all('div',class_='result__body')\n",
    "    contCambioPagina=contCambioPagina+1\n",
    "    cont=cont\n",
    "    for enlace in conten:\n",
    "        cont=cont+1\n",
    "\n",
    "        url = str(enlace.find_all('h2',class_='result-title'))\n",
    "        url = url.split('data-v-e393cff4=\"\" href=\"')\n",
    "        url = str(url[1])\n",
    "        url = url.split('\" rel=\"noopener\"')\n",
    "        url = str(url[0])\n",
    "\n",
    "        nombrePagina =str(enlace.find_all('div', class_='result__info'))\n",
    "        nombrePagina = nombrePagina.split('cff4=\"\">')\n",
    "        nombrePagina = str(nombrePagina[2])\n",
    "        nombrePagina = nombrePagina.split('</div>')\n",
    "        nombrePagina = str(nombrePagina[0])\n",
    "\n",
    "        titulo = str(enlace.find_all('h2',class_='result-title'))\n",
    "        titulo = titulo.split('target=\"_self\">')\n",
    "        titulo = str(titulo[1])\n",
    "        titulo = titulo.replace('</a> </h2>]','')\n",
    "\n",
    "        mensaje = str(enlace.find_all('', class_='news-result__description'))\n",
    "        mensaje= mensaje.split('data-v-e393cff4=\"\">')\n",
    "        mensaje= str(mensaje[1])\n",
    "        mensaje=mensaje.replace('</p>]', '')\n",
    "\n",
    "        fecha=str(enlace.find_all('time',class_='news-result__date'))\n",
    "        fecha=fecha.split('\">')\n",
    "        fecha=str(fecha[1])\n",
    "        fecha=fecha.replace('</time>]', '')\n",
    "    \n",
    "        presidenteNoticias = PresidenteNoticias(url =url,nombre_Pagina_WEB=nombrePagina,titulo = titulo, mensaje = mensaje, fecha = fecha).save()\n",
    "        browsersE.presidente.connect(presidenteNoticias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "#Palabras a Buscar\n",
    "urlAsambleista ='https://www.ecosia.org/news?q=andrea%20gonzales%20ecuador'\n",
    "\n",
    "#Encabezasos HTTP Para navegadores\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36'}\n",
    "\n",
    "cont=0\n",
    "contCambioPaginaE=0\n",
    "for i in range(1):\n",
    "    url = urlAsambleista+str(contCambioPaginaE)\n",
    "    respuesta = requests.get(url, headers = header )\n",
    "    contenido = BeautifulSoup(respuesta.content, 'html.parser')\n",
    "    conten = contenido.find_all('section', class_='news__results')\n",
    "    conten = contenido.find_all('div',class_='result__body')\n",
    "    contCambioPaginaE=contCambioPaginaE+1\n",
    "    cont=cont\n",
    "    for enlace in conten:\n",
    "        cont=cont+1\n",
    "\n",
    "        url = str(enlace.find_all('h2',class_='result-title'))\n",
    "        url = url.split('data-v-e393cff4=\"\" href=\"')\n",
    "        url = str(url[1])\n",
    "        url = url.split('\" rel=\"noopener\"')\n",
    "        url = str(url[0])\n",
    "\n",
    "        nombrePagina =str(enlace.find_all('div', class_='result__info'))\n",
    "        nombrePagina = nombrePagina.split('cff4=\"\">')\n",
    "        nombrePagina = str(nombrePagina[2])\n",
    "        nombrePagina = nombrePagina.split('</div>')\n",
    "        nombrePagina = str(nombrePagina[0])\n",
    "\n",
    "        titulo = str(enlace.find_all('h2',class_='result-title'))\n",
    "        titulo = titulo.split('target=\"_self\">')\n",
    "        titulo = str(titulo[1])\n",
    "        titulo = titulo.replace('</a> </h2>]','')\n",
    "\n",
    "        mensaje = str(enlace.find_all('', class_='news-result__description'))\n",
    "        mensaje= mensaje.split('data-v-e393cff4=\"\">')\n",
    "        mensaje= str(mensaje[1])\n",
    "        mensaje=mensaje.replace('</p>]', '')\n",
    "\n",
    "        fecha=str(enlace.find_all('time',class_='news-result__date'))\n",
    "        fecha=fecha.split('\">')\n",
    "        fecha=str(fecha[1])\n",
    "        fecha=fecha.replace('</time>]', '')\n",
    "        \n",
    "        asambleistaNoticia = AsambleistaNoticia(url = url,nombre_Pagina_WEB= nombrePagina,titulo = titulo, mensaje = mensaje, fecha = fecha).save()\n",
    "        browsersE.asambleista.connect(asambleistaNoticia)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "#Palabras a Buscar \n",
    "\n",
    "urlBingPresidente = 'https://www.bing.com/news/search?q=andres+arauz&FORM=NWRFSH'\n",
    "\n",
    "#Encabezasos HTTP Para navegadores\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36'}\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    url = urlBingPresidente\n",
    "    respuesta = requests.get(url, headers = header )\n",
    "    contenido = BeautifulSoup(respuesta.content, 'html.parser')\n",
    "    conten = contenido.find_all('div', class_='news-card newsitem cardcommon b_cards2')\n",
    "    print(\"Enlace: \")\n",
    "    for enlace in conten:\n",
    "\n",
    "        url = str(enlace.find_all('a'))\n",
    "        url = url.split('href=\"')\n",
    "        url = str(url[1])\n",
    "        url = url.split('tabindex=\"-1\"')\n",
    "        url = str(url[0])\n",
    "\n",
    "        nombrePagina =str(enlace.find_all('div', class_='source'))\n",
    "        nombrePagina = nombrePagina.split('\">')\n",
    "        nombrePagina = (nombrePagina[2])\n",
    "        nombrePagina = nombrePagina.replace('</a><span><span class=\"news-separator', '')\n",
    "\n",
    "        titulo = str(enlace.find_all('a',class_=\"title\"))\n",
    "        titulo = titulo.split('blank\">')\n",
    "        titulo = str(titulo[1])\n",
    "        titulo = titulo.replace('</a>]','')\n",
    "\n",
    "        mensaje = str(enlace.find_all('div', class_='snippet'))\n",
    "        mensaje= mensaje.split('\"')\n",
    "        mensaje= str(mensaje[3])\n",
    "\n",
    "        fecha=str(enlace.find_all('span'))\n",
    "        fecha=fecha.split('tabindex=\"0\">')\n",
    "        fecha=str(fecha[1])\n",
    "        fecha=fecha.replace('</span>]', '')\n",
    "        \n",
    "        presidenteNoticias = PresidenteNoticias(url =url,nombre_Pagina_WEB=nombrePagina,titulo = titulo, mensaje = mensaje, fecha = fecha).save()\n",
    "        browsersB.presidente.connect(presidenteNoticias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "#Palabras a Buscar \n",
    "\n",
    "#URL buscar en noticias sobre el Partido Politico UNES \n",
    "noticiasPartidoUNES = 'https://www.google.com/search?q=Partido+politico+UNES&tbm=nws&start='\n",
    "#Rango de paginas a buscar por Noticias del candidato a presidente Andr√©s Arauz \n",
    "rangoNoticiasPresidente = 18\n",
    "\n",
    "\n",
    "#Encabezasos HTTP Para navegadores\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36'}\n",
    "\n",
    "cont=0\n",
    "contID = 1;\n",
    "contCambioPaginaPartido=0\n",
    "for i in range(rangoNoticiasPresidente):\n",
    "    url = noticiasPartidoUNES+str(contCambioPaginaPartido)\n",
    "    respuesta = requests.get(url, headers = header )\n",
    "    contenido = BeautifulSoup(respuesta.content, \"html.parser\")\n",
    "    contenido = contenido.find_all('div', class_='dbsr')\n",
    "    \n",
    "    contCambioPaginaPartido=contCambioPaginaPartido+10\n",
    "    cont=cont\n",
    "    for lista in contenido:\n",
    "        cont=cont+1\n",
    "        \n",
    "        #SACAR URL\n",
    "        url = str(lista.find_all('a'))\n",
    "        url = url.split('href=\"')\n",
    "        url = str(url[1])\n",
    "        url = url.split('\" ping=\"')\n",
    "        url = str(url[0])\n",
    "        \n",
    "        #SACAR NOMBRE DE LA PAGINA WEB\n",
    "        nombreP =str(lista.find_all('div', class_='XTjFC WF4CUc'))\n",
    "        nombrePagina = nombreP.split('</g-img>')\n",
    "        nombrePagina = (nombrePagina[1])\n",
    "        nombrePagina = nombrePagina.replace('</div>]', '')\n",
    "        \n",
    "        #SACAR TITULO\n",
    "        titulo=str(lista.find_all('div', class_='JheGif nDgy9d'))\n",
    "        titulo=titulo.replace('[<div aria-level=\"2\" class=\"JheGif nDgy9d\" role=\"heading\" style=\"-webkit-line-clamp:2\">', '')\n",
    "        titulo=titulo.replace('</div>]', '')\n",
    "        \n",
    "        #SACAR MENSAJE\n",
    "        mensaje = str(lista.find_all('div', class_='Y3v8qd'))\n",
    "        mensaje=mensaje.replace('[<div class=\"Y3v8qd\">', '')\n",
    "        mensaje=mensaje.replace('</div>]', '')\n",
    "        \n",
    "        #SACAR FECHA\n",
    "        fecha=str(lista.find_all('span',class_='WG9SHc'))\n",
    "        fecha=fecha.replace('[<span class=\"WG9SHc\"><span>', '')\n",
    "        fecha=fecha.replace('</span></span>]', '')\n",
    "        \n",
    "        unes = UNES(url =url,nombre_Pagina_WEB=nombrePagina,titulo = titulo, mensaje = mensaje, fecha = fecha).save()\n",
    "        browsersG.unes.connect(unes)\n",
    "        \n",
    "        contID = contID + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "#Palabras a Buscar \n",
    "\n",
    "#URL buscar en noticias sobre el Partido Politico UNES \n",
    "noticiasPartidoUNES = 'https://www.google.com/search?q=Partido+UNES+AZUAY&tbm=nws&start='\n",
    "#Rango de paginas a buscar por Noticias del candidato a presidente Andr√©s Arauz \n",
    "rangoNoticiasPresidente = 18\n",
    "\n",
    "\n",
    "#Encabezasos HTTP Para navegadores\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36'}\n",
    "\n",
    "cont=0\n",
    "contID = 1;\n",
    "contCambioPaginaPartidoA=0\n",
    "for i in range(rangoNoticiasPresidente):\n",
    "    url = noticiasPartidoUNES+str(contCambioPaginaPartidoA)\n",
    "    respuesta = requests.get(url, headers = header )\n",
    "    contenido = BeautifulSoup(respuesta.content, \"html.parser\")\n",
    "    contenido = contenido.find_all('div', class_='dbsr')\n",
    "    \n",
    "    contCambioPaginaPartidoA=contCambioPaginaPartidoA+10\n",
    "    cont=cont\n",
    "    for lista in contenido:\n",
    "        cont=cont+1\n",
    "        \n",
    "        #SACAR URL\n",
    "        url = str(lista.find_all('a'))\n",
    "        url = url.split('href=\"')\n",
    "        url = str(url[1])\n",
    "        url = url.split('\" ping=\"')\n",
    "        url = str(url[0])\n",
    "        \n",
    "        #SACAR NOMBRE DE LA PAGINA WEB\n",
    "        nombreP =str(lista.find_all('div', class_='XTjFC WF4CUc'))\n",
    "        nombrePagina = nombreP.split('</g-img>')\n",
    "        nombrePagina = (nombrePagina[1])\n",
    "        nombrePagina = nombrePagina.replace('</div>]', '')\n",
    "        \n",
    "        #SACAR TITULO\n",
    "        titulo=str(lista.find_all('div', class_='JheGif nDgy9d'))\n",
    "        titulo=titulo.replace('[<div aria-level=\"2\" class=\"JheGif nDgy9d\" role=\"heading\" style=\"-webkit-line-clamp:2\">', '')\n",
    "        titulo=titulo.replace('</div>]', '')\n",
    "        \n",
    "        #SACAR MENSAJE\n",
    "        mensaje = str(lista.find_all('div', class_='Y3v8qd'))\n",
    "        mensaje=mensaje.replace('[<div class=\"Y3v8qd\">', '')\n",
    "        mensaje=mensaje.replace('</div>]', '')\n",
    "        \n",
    "        #SACAR FECHA\n",
    "        fecha=str(lista.find_all('span',class_='WG9SHc'))\n",
    "        fecha=fecha.replace('[<span class=\"WG9SHc\"><span>', '')\n",
    "        fecha=fecha.replace('</span></span>]', '')\n",
    "        \n",
    "        unesAzuay = UNESAZUAY(url =url,nombre_Pagina_WEB=nombrePagina,titulo = titulo, mensaje = mensaje, fecha = fecha).save()\n",
    "        browsersG.unesAzuay.connect(unesAzuay)\n",
    "        \n",
    "        contID = contID + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "#Palabras a Buscar \n",
    "\n",
    "urlBingUNES = 'https://www.bing.com/news/search?q=UNES'\n",
    "\n",
    "#Encabezasos HTTP Para navegadores\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36'}\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    url = urlBingUNES\n",
    "    respuesta = requests.get(url, headers = header )\n",
    "    contenido = BeautifulSoup(respuesta.content, 'html.parser')\n",
    "    conten = contenido.find_all('div', class_='news-card newsitem cardcommon b_cards2')\n",
    "    print(\"Enlace: \")\n",
    "    for enlace in conten:\n",
    "\n",
    "        url = str(enlace.find_all('a'))\n",
    "        url = url.split('href=\"')\n",
    "        url = str(url[1])\n",
    "        url = url.split('tabindex=\"-1\"')\n",
    "        url = str(url[0])\n",
    "\n",
    "        nombrePagina =str(enlace.find_all('div', class_='source'))\n",
    "        nombrePagina = nombrePagina.split('\">')\n",
    "        nombrePagina = (nombrePagina[2])\n",
    "        nombrePagina = nombrePagina.replace('</a><span><span class=\"news-separator', '')\n",
    "\n",
    "        titulo = str(enlace.find_all('a',class_=\"title\"))\n",
    "        titulo = titulo.split('blank\">')\n",
    "        titulo = str(titulo[1])\n",
    "        titulo = titulo.replace('</a>]','')\n",
    "\n",
    "        mensaje = str(enlace.find_all('div', class_='snippet'))\n",
    "        mensaje= mensaje.split('\"')\n",
    "        mensaje= str(mensaje[3])\n",
    "\n",
    "        fecha=str(enlace.find_all('span'))\n",
    "        fecha=fecha.split('tabindex=\"0\">')\n",
    "        fecha=str(fecha[1])\n",
    "        fecha=fecha.replace('</span>]', '')\n",
    "        unes = UNES(url =url,nombre_Pagina_WEB=nombrePagina,titulo = titulo, mensaje = mensaje, fecha = fecha).save()\n",
    "        browsersB.unes.connect(unes)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "#Palabras a Buscar \n",
    "\n",
    "urlBingUNEAZUAY = 'https://www.bing.com/news/search?q=UNES+AZUAY&qs=n&form=QBNT&sp=-1&pq=unes+azua&sc=8-9&sk=&cvid=0E968B2B4A54407680C679A971205835'\n",
    "\n",
    "#Encabezasos HTTP Para navegadores\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36'}\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    url = urlBingUNEAZUAY\n",
    "    respuesta = requests.get(url, headers = header )\n",
    "    contenido = BeautifulSoup(respuesta.content, 'html.parser')\n",
    "    conten = contenido.find_all('div', class_='news-card newsitem cardcommon b_cards2')\n",
    "    print(\"Enlace: \")\n",
    "    for enlace in conten:\n",
    "\n",
    "        url = str(enlace.find_all('a'))\n",
    "        url = url.split('href=\"')\n",
    "        url = str(url[1])\n",
    "        url = url.split('tabindex=\"-1\"')\n",
    "        url = str(url[0])\n",
    "\n",
    "        nombrePagina =str(enlace.find_all('div', class_='source'))\n",
    "        nombrePagina = nombrePagina.split('\">')\n",
    "        nombrePagina = (nombrePagina[2])\n",
    "        nombrePagina = nombrePagina.replace('</a><span><span class=\"news-separator', '')\n",
    "\n",
    "        titulo = str(enlace.find_all('a',class_=\"title\"))\n",
    "        titulo = titulo.split('blank\">')\n",
    "        titulo = str(titulo[1])\n",
    "        titulo = titulo.replace('</a>]','')\n",
    "\n",
    "        mensaje = str(enlace.find_all('div', class_='snippet'))\n",
    "        mensaje= mensaje.split('\"')\n",
    "        mensaje= str(mensaje[3])\n",
    "\n",
    "        fecha=str(enlace.find_all('span'))\n",
    "        fecha=fecha.split('tabindex=\"0\">')\n",
    "        fecha=str(fecha[1])\n",
    "        fecha=fecha.replace('</span>]', '')\n",
    "        unesAzuay = UNESAZUAY(url =url,nombre_Pagina_WEB=nombrePagina,titulo = titulo, mensaje = mensaje, fecha = fecha).save()\n",
    "        browsersB.unesAzuay.connect(unesAzuay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "#Palabras a Buscar \n",
    "\n",
    "#URL buscar en noticias sobre el Partido Politico UNES \n",
    "urlfacebookPresidente = 'https://www.google.com/search?q=https://www.facebook.com/ecuarauz2021&start='\n",
    "#Rango de paginas a buscar por Noticias del candidato a presidente Andr√©s Arauz \n",
    "rangourlfacebookPresidente = 8\n",
    "\n",
    "\n",
    "#Encabezasos HTTP Para navegadores\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36'}\n",
    "\n",
    "cont=0\n",
    "contID = 1;\n",
    "contCambioPaginaPartidoA=30\n",
    "for i in range(rangourlfacebookPresidente):\n",
    "    url = urlfacebookPresidente+str(contCambioPaginaPartidoA)\n",
    "    respuesta = requests.get(url, headers = header )\n",
    "   \n",
    "    contenido = BeautifulSoup(respuesta.content, \"html.parser\")\n",
    "\n",
    "    contenido = contenido.find_all('div', class_=\"g\")\n",
    "    contCambioPaginaPartidoA=contCambioPaginaPartidoA+10\n",
    "    cont=cont\n",
    "    for lista in contenido:\n",
    "        cont=cont+1\n",
    "        \n",
    "        #SACAR URL\n",
    "        url = str(lista.find_all('a'))\n",
    "        url = url.split('href=\"')\n",
    "        url = str(url[1])\n",
    "        url = url.split('\" ping=\"')\n",
    "        url = str(url[0])\n",
    "        \n",
    "        \n",
    "        #SACAR TITULO DE LA PAGINA WEB \n",
    "        titulo =str(lista.find_all('div', class_='TbwUpd NJjxre'))\n",
    "        titulo = titulo.split('tjvcx\">')\n",
    "        titulo = (titulo[1])\n",
    "        titulo = titulo.split('<span')\n",
    "        titulo = (titulo[0])\n",
    "        \n",
    "        #SACAR NOMBRE DE LA PAGINA WEB \n",
    "        nombrePagina=str(lista.find_all('h3', class_='LC20lb DKV0Md'))\n",
    "        nombrePagina = nombrePagina.split('<span>')\n",
    "        nombrePagina = (nombrePagina[1])\n",
    "        nombrePagina = nombrePagina.split('</span>')\n",
    "        nombrePagina = (nombrePagina[0])\n",
    "        \n",
    "        #SACAR MENSAJE\n",
    "        mensaje = str(lista.find_all('span', class_='aCOpRe'))\n",
    "        mensaje = mensaje.split('<span>')\n",
    "        mensaje = (mensaje[1])\n",
    "        mensaje=mensaje.replace('</span></span>]', '')\n",
    "        mensaje=mensaje.replace('<em>', '')\n",
    "        mensaje=mensaje.replace('</em>', '')\n",
    "        \n",
    "        \n",
    "        presidenteFacebook = PresidenteFacebook(url =url,nombre_Pagina_WEB=nombrePagina,titulo = titulo, mensaje = mensaje).save()\n",
    "        browsersG.presidenteFacebook.connect(presidenteFacebook)\n",
    "        \n",
    "        contID = contID + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/search?q=https://www.facebook.com/AndreGonzalezBe&start=30\n",
      "https://twitter.com/veronicatama\n",
      "twitter.com\n",
      "VERONICA TAMA (@veronicatama) | Twitter\n",
      "... Sobre los derechos humanos vulnerados en la pandemia @<wbr/>GPDerechoHumano @dorissoliz https://t.co/LfMYrBL5fI\" ... Andrea Gonzalez @<wbr/>AndreGonzalezBe.\n",
      "https://www.pictame.com/user/fernanda8ap/followings/539794342\n",
      "www.pictame.com\n",
      "Fernanda Ochoa (@fernanda8ap) Followings | Instagram ...\n",
      "ERASMUS IN ALMER√çA   Ecuatoriana TURISMO- FB: Una ecuatoriana viajando por el mundo      21 pa√≠ses‚úàÔ∏è AFU  2015-2017 Fotograf√≠a  ¬†...\n",
      "https://www.ebay.es/itm/TOSTADORA-TOSTADOR-METALICA-2-RANURAS-LARGAS-4-TOSTADAS-REBANADAS-METAL-GARANTIA-/281233478773\n",
      "www.ebay.es\n",
      "TOSTADORA TOSTADOR METALICA 2 RANURAS LARGAS ...\n",
      "por andregonzalezbe... 02 dic 2018. Bien. X. Imagen anterior. Imagen siguiente. Compra verificada: S√≠ | Estado: nuevo | Vendido por: lbc-logistics. (0). (0).\n",
      "https://www.trendsmap.com/twitter/tweet/1339771552037036033\n",
      "www.trendsmap.com\n",
      "Andrea Gonzalez's tweet - \"Recibimos con entusiasmo a ...\n",
      "@AndreGonzalezBe | 602 followers. Recibimos con entusiasmo a @<wbr/>rabascallcarlos en Azuay. ¬°Vamos a recuperar el futuro! ‚òùüß°1Ô∏è‚É£\n",
      "http://www.bristolpress.com/BP-Bristol+Central+Sports/380925/roundup-southington-boys-soccer-gets-closer-to-postseason-berth-after-win-over-avon\n",
      "www.bristolpress.com\n",
      "Roundup: Southington boys soccer gets ... - The Bristol Press\n",
      "Facebook ¬∑ Twitter ... Goals: Scott Groleau (LM) 2, Andre Gonzalez (BE), Chris King (LM), Adam Borry (BE). Assists: Conner Policarpio (BE).\n",
      "https://www.globalnpo.org/XX/Unknown/102941133539703/Corporaci%C3%B3n-Juventud-Independiente\n",
      "www.globalnpo.org\n",
      "Corporaci√≥n Juventud Independiente (2020) - GlobalNPO\n",
      "... @unesazuay Juan Crist√≥bal Lloret @jotalloretv y Andrea Gonz√°lez @<wbr/>andregonzalezbe. ... Inscr√≠bete en: https://forms.gle/yCU2TGUwvrtz7a2p8 ... A partir de las 19h30 lo transmitiremos por Facebook Live, nos acompa√±ar√°n Sofia Pretti,¬†...\n",
      "http://173.232.228.167/index.pl/id/00/https/twitter.com/vocesdelsurec=3flang=3dno\n",
      "173.232.228.167\n",
      "Voces Del Sur (@VocesDelSurEC) | Twitter\n",
      "Sitio Web de Noticias, aqu√≠ te contaremos la verdad, lo que no te dicen los medios de comunicaci√≥n tradicionales. Instagram: https://t.co/uTD1eG0lBD.\n",
      "http://173.232.228.167/index.pl/id/00/https/twitter.com/ivanabrilrc=3flang=3dko\n",
      "173.232.228.167\n",
      "Iv√°n Abril(@IvanAbrilRC) Îãò | Ìä∏ÏúÑÌÑ∞\n",
      "... salas de los juzgados, con dolorosas consecuencias para el Estado de Derecho‚Äù la opini√≥n de @ramiroaguilart https://go.shr.lc/2DHjxmH v√≠a @<wbr/>radiolacalle.\n",
      "https://stweetly.com/ar/AuriArias16/\n",
      "stweetly.com\n",
      "Aura Arias (@AuriArias16) Twitter ÿ≠ÿ≥ÿßÿ® ‚Ä¢ sTweetly\n",
      "Estreno mi√©rcoles 18:30pm v√≠a Facebook live RC Compromiso para recuperar la Patria somosRC twitter.com/RutaKritica/st‚Ä¶ October 12, 2020.\n",
      "https://stweetly.com/ja/jotalloretv/\n",
      "stweetly.com\n",
      "Jota Lloret (@jotalloretv) „ÉÑ„Ç§„ÉÉ„Çø„Éº „Ç¢„Ç´„Ç¶„É≥„Éà ‚Ä¢ sTweetly\n",
      "Android : #EnVivo | Debate de Jota Lloret, Marcelo Cabrera y Bruno Segovia, candidatos a Asamble√≠stas por el Azuay Link: m.facebook.com/story.php?stor‚Ä¶\n",
      "https://www.google.com/search?q=https://www.facebook.com/AndreGonzalezBe&start=40\n",
      "https://nitter.nixnet.services/lduranaguilar\n",
      "nitter.nixnet.services\n",
      "Liliana Dur√°n Aguilar (@LilianaDuranA) | nitter\n",
      "... el trabajo unificado de las organizaciones pol√≠ticas y sociales del progresismo ecuatoriano y el proceso para recuperar la Patria. fb.watch/1B5Hd1MhST/. 0. 5.\n",
      "https://igdig.com/tw/tag/azuay/recent\n",
      "igdig.com\n",
      "„ÄåAzuay„ÄçInstagram photo and video - igDig\n",
      "Brazil ¬∑ Russia ¬∑ Germany ¬∑ United Kingdom ¬∑ Taiwan ¬∑ Hong Kong ¬∑ Instagram ¬∑ Facebook ¬∑ Twitter ¬∑ Youtube. Copyright ¬© 2020 igDig, All Rights Reserved.\n",
      "https://www.google.com/search?q=https://www.facebook.com/AndreGonzalezBe&start=50\n",
      "https://www.google.com/search?q=https://www.facebook.com/AndreGonzalezBe&start=60\n",
      "https://www.google.com/search?q=https://www.facebook.com/AndreGonzalezBe&start=70\n",
      "https://www.google.com/search?q=https://www.facebook.com/AndreGonzalezBe&start=80\n",
      "https://www.google.com/search?q=https://www.facebook.com/AndreGonzalezBe&start=90\n",
      "https://www.google.com/search?q=https://www.facebook.com/AndreGonzalezBe&start=100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "#Palabras a Buscar \n",
    "\n",
    "#URL buscar en noticias sobre el Partido Politico UNES \n",
    "facebookAsamblea = 'https://www.google.com/search?q=https://www.facebook.com/AndreGonzalezBe&start='\n",
    "#Rango de paginas a buscar por Noticias del candidato a presidente Andr√©s Arauz \n",
    "rangofacebookAsamblea = 8\n",
    "\n",
    "\n",
    "#Encabezasos HTTP Para navegadores\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36'}\n",
    "\n",
    "cont=0\n",
    "contID = 1;\n",
    "contCambioPaginaPartidoA=30\n",
    "for i in range(rangofacebookAsamblea):\n",
    "    url = facebookAsamblea+str(contCambioPaginaPartidoA)\n",
    "    respuesta = requests.get(url, headers = header )\n",
    "   \n",
    "    contenido = BeautifulSoup(respuesta.content, \"html.parser\")\n",
    "\n",
    "    contenido = contenido.find_all('div', class_=\"g\")\n",
    "    contCambioPaginaPartidoA=contCambioPaginaPartidoA+10\n",
    "    cont=cont\n",
    "    print(url)\n",
    "    for lista in contenido:\n",
    "        cont=cont+1\n",
    "        \n",
    "        #SACAR URL\n",
    "        url = str(lista.find_all('a'))\n",
    "        url = url.split('href=\"')\n",
    "        url = str(url[1])\n",
    "        url = url.split('\" ping=\"')\n",
    "        url = str(url[0])\n",
    "        \n",
    "        \n",
    "        #SACAR TITULO DE LA PAGINA WEB \n",
    "        titulo =str(lista.find_all('div', class_='TbwUpd NJjxre'))\n",
    "        titulo = titulo.split('tjvcx\">')\n",
    "        titulo = (titulo[1])\n",
    "        titulo = titulo.split('<span')\n",
    "        titulo = (titulo[0])\n",
    "        \n",
    "        #SACAR NOMBRE DE LA PAGINA WEB \n",
    "        nombrePagina=str(lista.find_all('h3', class_='LC20lb DKV0Md'))\n",
    "        nombrePagina = nombrePagina.split('<span>')\n",
    "        nombrePagina = (nombrePagina[1])\n",
    "        nombrePagina = nombrePagina.split('</span>')\n",
    "        nombrePagina = (nombrePagina[0])\n",
    "        \n",
    "        #SACAR MENSAJE\n",
    "        mensaje = str(lista.find_all('span', class_='aCOpRe'))\n",
    "        mensaje = mensaje.split('<span>')\n",
    "        mensaje = (mensaje[1])\n",
    "        mensaje=mensaje.replace('</span></span>]', '')\n",
    "        mensaje=mensaje.replace('<em>', '')\n",
    "        mensaje=mensaje.replace('</em>', '')\n",
    "        \n",
    "        #SACAR FECHA\n",
    "        #fecha=str(lista.find_all('span',class_='WG9SHc'))\n",
    "        #fecha=fecha.replace('[<span class=\"WG9SHc\"><span>', '')\n",
    "        #fecha=fecha.replace('</span></span>]', '')\n",
    "        \n",
    "        asambleaFacebook = AsambleistaFacebook(url =url,nombre_Pagina_WEB=nombrePagina,titulo = titulo, mensaje = mensaje).save()\n",
    "        browsersG.asambleistaFacebook.connect(asambleaFacebook)\n",
    "        \n",
    "        contID = contID + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
